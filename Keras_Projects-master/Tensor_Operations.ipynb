{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Representation \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import models, layers\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels)= mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors are the basic data structure for deep learning\n",
    "#### Scalar- 0D Tensor\n",
    "#### Vector- 1D Tensor\n",
    "#### Matrix- 2D Tensor\n",
    "#### Array- 3D or higher-dimnesional Tensors\n",
    "\n",
    "\n",
    "#### The number of axes of the tensor is called *rank*\n",
    "#### Dimensionality can denote either the number of entries along a specific axis (as in the case of a 5D vector) or the number of axes in a tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scalars- \n",
    "   #### A tensor that contains only 1 number\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.array(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(12)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectors-\n",
    "#### An array of numbers is called vector. A vector that has 4 entries is called a 4D vector but it is not a 4D Tensor. It has only one axis but 5 dimensions along the axis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= np.array([12,4,5,7,8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12,  4,  5,  7,  8])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrices-\n",
    "\n",
    "#### An array of vector is called matrix or 2D Tensor. The 2 axes are *rows* and *columns*. The entirs from the first axis are called *rows* and the entries from the second axis are called *columns*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= np.array([[1,2,3,4,5],[76,7887,947,4,6],[45,2,6,3,8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1,    2,    3,    4,    5],\n",
       "       [  76, 7887,  947,    4,    6],\n",
       "       [  45,    2,    6,    3,    8]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3DTensors-\n",
    "#### Multiple matrires pakced in an array is called 3D Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.array([[\n",
    "    [1,2,3,4,5],\n",
    "    [234,6,7,8,6]],\n",
    "    [[1,2,3,4,5],\n",
    "     [1,2,3,4,5]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  1,   2,   3,   4,   5],\n",
       "        [234,   6,   7,   8,   6]],\n",
       "\n",
       "       [[  1,   2,   3,   4,   5],\n",
       "        [  1,   2,   3,   4,   5]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Attributes:\n",
    "A tensor is defined by 3 attributes-\n",
    " - Rank (number of axes): \n",
    " - Shape\n",
    " - Data type "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('uint8')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit= train_images[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(digit, cmap= plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulate Tensors using Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting a specific element in a tensor is called ***Tensor Slicing***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_slice=train_images[10:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90, 28, 28)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_slice.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_slice=train_images[10:100,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_slice=train_images[10:100,0:28, 0:28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90, 28, 28)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_slice.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_slice= train_images[10:100, 10:24,10:24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(tensor_slice[7], cmap= plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_slice= train_images[10:100, 7:-7,7:-7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(tensor_slice[7], cmap= plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\":\"** is equivalent to selecting the entire axis. It can also be used to crop images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first axis (axis 0) in all data tensors is ***sample axis***. Deep Learning models dont train the data at once, \n",
    "it does it in batches.\n",
    "While consiedring ***batch tensor***, the first axis is called ***batch axis*** or ***batch dimension***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 28, 28)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch= train_images[:128]\n",
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 28, 28)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch= train_images[128:256]\n",
    "batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For nth batch\n",
    "\n",
    "```batch= train_images[128*n:128*(n+1)]\n",
    "batch.shape```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real world examples of data tensors:\n",
    " - **Vector Data:** 2D tensors of shape *(sample, features)*\n",
    "> e.g. - An actuarial dataset of people, where we consider each person’s age, ZIP code,\n",
    "and income. Each person can be characterized as a vector of 3 values, and thus\n",
    "an entire dataset of 100,000 people can be stored in a 2D tensor of shape\n",
    "(100000, 3).\n",
    " - **Time Series Data/ Sequence Data:** 3D tensors of shape *(samples, timesteps, features)*\n",
    " ![capture](https://user-images.githubusercontent.com/13174586/49436810-0e553400-f7e0-11e8-8b9a-586f23360a3c.JPG)\n",
    "> e.g. - A dataset of stock prices. Every minute, we store the current price of the stock,\n",
    "the highest price in the past minute, and the lowest price in the past minute.\n",
    "Thus every minute is encoded as a 3D vector, an entire day of trading is\n",
    "encoded as a 2D tensor of shape (390, 3) (there are 390 minutes in a trading\n",
    "day), and 250 days’ worth of data can be stored in a 3D tensor of shape (250,\n",
    "390, 3). Here, each sample would be one day’s worth of data \n",
    " - **Images:** 4D tensors of shape *(sample, height, width, channel)/(sample, channel, height, width)*\n",
    " ![capture](https://user-images.githubusercontent.com/13174586/49436988-83c10480-f7e0-11e8-841a-e132f026f541.JPG)\n",
    "> Images typically have three dimensions: height, width, and color depth. Although\n",
    "grayscale images (like our MNIST digits) have only a single color channel and could\n",
    "thus be stored in 2D tensors, by convention image tensors are always 3D, with a onedimensional\n",
    "color channel for grayscale images. A batch of 128 grayscale images of\n",
    "size 256 × 256 could thus be stored in a tensor of shape (128, 256, 256, 1), and a\n",
    "batch of 128 color images could be stored in a tensor of shape (128, 256, 256, 3)\n",
    " - **Video:** 4D tensors of shape *(sample, frames, height, width, channel)/(sample, frames, channel, height, width)*\n",
    "> Video data is one of the few types of real-world data for which you’ll need 5D tensors.\n",
    "A video can be understood as a sequence of frames, each frame being a color image.\n",
    "Because each frame can be stored in a 3D tensor (height, width, color_depth), a\n",
    "sequence of frames can be stored in a 4D tensor (frames, height, width, color_\n",
    "depth), and thus a batch of different videos can be stored in a 5D tensor of shape\n",
    "(samples, frames, height, width, color_depth).\n",
    "<br/>\n",
    "e.g.- For instance, a 60-second, 144 × 256 YouTube video clip sampled at 4 frames per\n",
    "second would have 240 frames. A batch of four such video clips would be stored in a\n",
    "tensor of shape (4, 240, 144, 256, 3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Operations\n",
    "\n",
    "```keras.layers.Dense(512, activation='relu')\n",
    "op= relu(dot(W,ip)+b)```\n",
    "\n",
    "where W is a 2D tensor and b is a vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Element-wise operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```def naive_relu(x):\n",
    "    assert len(x.shape)==2\n",
    "    x=x.copy()\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i,j]= max(x[i,j],0)\n",
    "    return x```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```def naive_add(x,y):\n",
    "    assert len(x.shape)==2    \n",
    "    x=x.copy()\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i,j]+=y[i,j]\n",
    "    return x```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same above operations using ***Numpy***\n",
    "\n",
    "```import numpy as np\n",
    "z=z+y\n",
    "z= np.maximum(x,0)```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcasting\n",
    "\n",
    "When the addition of two tensors of different shapes, we use ***broadcasting***\n",
    "\n",
    "When possible and if there’s no ambiguity, the smaller tensor will be broadcasted to match the shape of the larger tensor. <br/>Broadcasting consists of two steps:\n",
    " - Axes (called *broadcast axes*) are added to the smaller tensor to match the ndim of the larger tensor.\n",
    " - The smaller tensor is repeated alongside these new axes to match the full shape of the larger tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lab= train_labels.reshape(1,60000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 60000)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(train_lab[0][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lab= np.append(train_lab,train_lab, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 60000)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5, 0, 4, ..., 5, 6, 8],\n",
       "       [5, 0, 4, ..., 5, 6, 8]], dtype=uint8)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_lab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_relu(x):\n",
    "    assert len(x.shape)==2\n",
    "    x=x.copy()\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i,j]= max(x[i,j],0)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_add(x,y):\n",
    "    assert len(x.shape)==2    \n",
    "    x=x.copy()\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i,j]+=y[i,j]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab= naive_add(train_lab, train_lab)\n",
    "ac= naive_relu(ab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10,  0,  8, ..., 10, 12, 16],\n",
       "       [10,  0,  8, ..., 10, 12, 16]], dtype=uint8)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With broadcasting, we can generally apply two-tensor element-wise operations if one\n",
    "tensor has shape ***(a, b, … n, n + 1, … m)*** and the other has shape ***(n, n + 1, … m)***. The\n",
    "broadcasting will then automatically happen for axes a through ***n - 1***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= np.random.random((3,2,3,2))\n",
    "b= np.random.random((2,3,2))\n",
    "c= np.maximum(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[0.97714992 0.97490223]\n",
      "   [0.09981567 0.1246505 ]\n",
      "   [0.00620382 0.68450016]]\n",
      "\n",
      "  [[0.77242125 0.1054652 ]\n",
      "   [0.29980982 0.96122253]\n",
      "   [0.81511276 0.80715693]]]\n",
      "\n",
      "\n",
      " [[[0.21208335 0.68096456]\n",
      "   [0.28275271 0.30352589]\n",
      "   [0.16752363 0.20076329]]\n",
      "\n",
      "  [[0.82312877 0.32818145]\n",
      "   [0.05544286 0.15156637]\n",
      "   [0.56891153 0.71820486]]]\n",
      "\n",
      "\n",
      " [[[0.80103848 0.8048053 ]\n",
      "   [0.61495057 0.36180157]\n",
      "   [0.89546584 0.81025779]]\n",
      "\n",
      "  [[0.57518859 0.24818846]\n",
      "   [0.03835998 0.76346252]\n",
      "   [0.37944881 0.93651953]]]]\n",
      "4\n",
      "(3, 2, 3, 2)\n",
      "[[[0.70872432 0.20195569]\n",
      "  [0.0428356  0.23570297]\n",
      "  [0.29597288 0.04376766]]\n",
      "\n",
      " [[0.5069044  0.76808839]\n",
      "  [0.00967405 0.38740428]\n",
      "  [0.65596643 0.56966011]]]\n",
      "[[[[0.97714992 0.97490223]\n",
      "   [0.09981567 0.23570297]\n",
      "   [0.29597288 0.68450016]]\n",
      "\n",
      "  [[0.77242125 0.76808839]\n",
      "   [0.29980982 0.96122253]\n",
      "   [0.81511276 0.80715693]]]\n",
      "\n",
      "\n",
      " [[[0.70872432 0.68096456]\n",
      "   [0.28275271 0.30352589]\n",
      "   [0.29597288 0.20076329]]\n",
      "\n",
      "  [[0.82312877 0.76808839]\n",
      "   [0.05544286 0.38740428]\n",
      "   [0.65596643 0.71820486]]]\n",
      "\n",
      "\n",
      " [[[0.80103848 0.8048053 ]\n",
      "   [0.61495057 0.36180157]\n",
      "   [0.89546584 0.81025779]]\n",
      "\n",
      "  [[0.57518859 0.76808839]\n",
      "   [0.03835998 0.76346252]\n",
      "   [0.65596643 0.93651953]]]]\n",
      "4\n",
      "(3, 2, 3, 2)\n"
     ]
    }
   ],
   "source": [
    "print(a)\n",
    "print(a.ndim)\n",
    "print(a.shape)\n",
    "print(b)\n",
    "print(c)\n",
    "print(c.ndim)\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Dot\n",
    "The dot operation, also called a ***tensor product*** (not to be confused with an elementwise\n",
    "product) is the most common, most useful tensor operation. Contrary to\n",
    "element-wise operations, it combines entries in the input tensors.\n",
    " An element-wise product is done with the * operator in Numpy, Keras, Theano,\n",
    "and TensorFlow. dot uses a different syntax in TensorFlow, but in both Numpy and\n",
    "Keras it’s done using the standard dot operator\n",
    "\n",
    "![capture1](https://user-images.githubusercontent.com/13174586/49464343-a4a94a00-f820-11e8-9464-d5fece84ed84.JPG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "o= np.random.random((3,3))\n",
    "p= np.random.random((3,2))\n",
    "q= np.dot(o,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9746623  0.94866803 0.55324458]\n",
      " [0.39040379 0.11327864 0.98007907]\n",
      " [0.07665248 0.79152354 0.24759697]]\n",
      "[[0.64264033 0.16917083]\n",
      " [0.79446666 0.41918494]\n",
      " [0.38422403 0.89978331]]\n",
      "[[1.59261227 1.06035202]\n",
      " [0.71745525 0.99538843]\n",
      " [0.77323175 0.56754574]]\n"
     ]
    }
   ],
   "source": [
    "print(o)\n",
    "print(p)\n",
    "print(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(o.shape[1])\n",
    "print(p.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "a= np.array([[0,3,5],\n",
    "            [5,5,2]])\n",
    "b= np.array([[3,4,1],\n",
    "            [3,-2,2],\n",
    "            [4,-2,4]])\n",
    "print(len(a.shape))\n",
    "print(len(b.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape[1]==b.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 29, -16,  26],\n",
       "       [ 38,   6,  23]])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_prod(x,y):\n",
    "    assert len(x.shape)==2\n",
    "    assert len(y.shape)==2\n",
    "    assert x.shape[1]==y.shape[0]\n",
    "    z= np.zeros((3,3))\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            z[i,j]+= x[i,j]*y[i,j] \n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.,  12.,   5.],\n",
       "       [ 15., -10.,   4.],\n",
       "       [  0.,   0.,   0.]])"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot_prod(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0.])"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros(a.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Reshaping\n",
    "Reshaping a tensor means rearranging its rows and columns to match a target shape.\n",
    "Naturally, the reshaped tensor has the same total number of coefficients as the initial\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= np.array([[0,1],\n",
    "            [2,3],\n",
    "            [4,5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1]\n",
      " [2 3]\n",
      " [4 5]]\n",
      "[[0 1 2]\n",
      " [3 4 5]]\n",
      "[[0 1 2 3 4 5]]\n",
      "[[0]\n",
      " [1]\n",
      " [2]\n",
      " [3]\n",
      " [4]\n",
      " [5]]\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "print(x.reshape(2,3))\n",
    "print(x.reshape(1,6))\n",
    "print(x.reshape(6,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A special case of reshaping that’s commonly encountered is transposition. Transposing a\n",
    "matrix means exchanging its rows and its columns, so that x[i, :] becomes x[:, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 200)\n",
      "(200, 20)\n"
     ]
    }
   ],
   "source": [
    "x= np.zeros((20,200))\n",
    "print(x.shape)\n",
    "y= np.transpose(x)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geometric Interpretation of Tensors Operations\n",
    "Because the contents of the tensors manipulated by tensor operations can be interpreted\n",
    "as coordinates of points in some geometric space, all tensor operations have a\n",
    "geometric interpretation. For instance, let’s consider addition. We’ll start with the following\n",
    "vector:\n",
    "```A = [0.5, 1]```\n",
    "It’s a point in a 2D space. It’s common to picture a vector as an arrow\n",
    "linking the origin to the point\n",
    "![capture](https://user-images.githubusercontent.com/13174586/49464807-f7cfcc80-f821-11e8-882c-49927c1a4547.JPG)\n",
    "![capture1](https://user-images.githubusercontent.com/13174586/49464805-f7373600-f821-11e8-8f94-7873856ad97b.JPG)\n",
    "\n",
    "Let’s consider a new point, ```B = [1, 0.25]```, which we’ll add to the previous one. This is\n",
    "done geometrically by chaining together the vector arrows, with the resulting location\n",
    "being the vector representing the sum of the previous two vectors\n",
    "![capture](https://user-images.githubusercontent.com/13174586/49464922-3a91a480-f822-11e8-9be4-9f6d3b881d2a.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geometric Interpretation of Deep Learning\n",
    "Neural networks consist entirely of chains of tensor operations and\n",
    "that all of these tensor operations are just geometric transformations of the input data.\n",
    "It follows that you can interpret a neural network as a very complex geometric transformation\n",
    "in a high-dimensional space, implemented via a long series of simple steps.\n",
    "\n",
    "In 3D, the following mental image may prove useful. Imagine two sheets of colored\n",
    "paper: one red and one blue. Put one on top of the other. Now crumple them\n",
    "together into a small ball. That crumpled paper ball is your input data, and each sheet\n",
    "of paper is a class of data in a classification problem. What a neural network (or any\n",
    "other machine-learning model) is meant to do is figure out a transformation of the\n",
    "paper ball that would uncrumple it, so as to make the two classes cleanly separable\n",
    "again. With deep learning, this would be implemented as a series of simple transformations\n",
    "of the 3D space, such as those you could apply on the paper ball with your fingers,\n",
    "one movement at a time.\n",
    "\n",
    "Uncrumpling paper balls is what machine learning is about: finding neat representations\n",
    "for complex, highly folded data manifolds. Deep Learning takes the approach of\n",
    "incrementally decomposing a complicated geometric transformation into a long\n",
    "chain of elementary ones, which is pretty much the strategy a human would follow to\n",
    "uncrumple a paper ball. Each layer in a deep network applies a transformation that\n",
    "disentangles the data a little—and a deep stack of layers makes tractable an extremely\n",
    "complicated disentanglement process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Based Optimization\n",
    "\n",
    "Each neural layer transforms the data as follows:\n",
    "\n",
    "```output= relu(dot(W,input)+b)```\n",
    "\n",
    "Here, ```W``` is ***Weight/Kernel*** and ```b``` is ***bias***. Both are ***trainable parameters*** of the layers. At first, ```W``` is filled with random values using ***random Initialization***, then with *training* using the feedback signal, ```W``` is adjusted. \n",
    "\n",
    "Steps of ***Training Loop***: \n",
    "\n",
    " - Batch of training sample ```x``` and corresponding training target ```y``` are selected\n",
    " - Run the model using ***forward pass*** on ```x``` for get ```y_pred```\n",
    " - Compute the loss of the network ```y-y_pred```\n",
    " - Update the weights of the model and keep retarining to reduce **loss**\n",
    " \n",
    "### Derivative\n",
    "\n",
    "Let us consider a smooth function ```f(x)=y```. Since the function is *continuous*, a small change in ```x (delta_x)```, will result in a small change in ```y (delta_y)```.\n",
    "\n",
    "```f(x + delta_x) = y + delta_y```\n",
    "\n",
    "![capture](https://user-images.githubusercontent.com/13174586/49468444-e7701f80-f82a-11e8-8974-e0e2acd9cfbe.JPG)\n",
    "\n",
    "Since the function is *smooth*, at cetrain point *p*, the ```delta_x``` is so small, that the function ```f(x)``` becomes linear with a slope of ```a```. Thus,\n",
    "\n",
    "```f(x + delta_x) = y + a*delta_y```\n",
    "\n",
    "The slope ```a``` is called the ***derivative*** of ```f``` in *p*. \n",
    "\n",
    "If ```a``` is ***negative*** , ```delta_x``` at point *p*  will decrease ```f(x)```. \n",
    "\n",
    "If ```a``` is ***positive*** , ```delta_x``` at point *p*  will increase ```f(x)```. \n",
    "\n",
    "The ***absolute value/ magnitude*** of ```a``` will depict how quickly the value  ```f(x)``` will increase or decrease.\n",
    "\n",
    "e.g.-\n",
    "\n",
    "```f(x)= a*cos(x)```<br/>\n",
    "```f'(x)= -a*sin(x)```\n",
    "\n",
    "### Gradient (Derivative) of Tensor Operation\n",
    "A *gradient* is the *derivative* of Tensor Operation. \n",
    "\n",
    "\n",
    "**Input Vector:** x<br/>\n",
    "**Weight:** W<br/>\n",
    "**Target:** y<br/>\n",
    "**Loss Function:** loss<br/>\n",
    "\n",
    "`y_pred= dot(W,x)\n",
    "loss_value= loss(y, y_pred)\n",
    "`\n",
    "<br/>\n",
    "\n",
    "Let’s say the current value of ```W``` is ```W0```. Then the derivative of ```f``` in the point ```W0``` is a tensor\n",
    "```gradient(f)(W0)``` with the same shape as ```W```, where each coefficient ```gradient(f)(W0)[i, j]``` indicates the direction and magnitude of the change in loss_value we observe when modifying ```W0[i, j]```. That tensor ```gradient(f)(W0)``` is the gradient of the function ```f(W) = loss_value``` in ```W0```.\n",
    "\n",
    "The derivative of a function ```f(x)``` of a single coefficient can be\n",
    "interpreted as the *slope* of the curve of ```f```. Likewise, ```gradient(f)(W0)``` can be interpreted\n",
    "as the tensor describing the *curvature* of ```f(W)``` around ```W0```.\n",
    "\n",
    "We can reduce the value of ```f(x)``` by moving ```x``` a little in the opposite direction from the derivative,\n",
    "with a function ```f(W)``` of a tensor, we can reduce ```f(W)``` by moving ```W``` in the opposite\n",
    "direction from the gradient: for example, ```W1 = W0 - step * gradient(f)(W0)``` (where\n",
    "*step* is a *small scaling factor*). That means going against the curvature, which intuitively\n",
    "should put us lower on the curve. Note that the scaling factor step is needed\n",
    "because ```gradient(f)(W0)``` only approximates the curvature when we’re close to ```W0```,\n",
    "so we don’t want to get too far from ```W0```. \n",
    "\n",
    "### Stochastic Gradient Descent\n",
    "\n",
    "Given a differentiable function, it’s theoretically possible to find its minimum analytically:\n",
    "it’s known that a function’s minimum is a point where the derivative is 0, so all\n",
    "we have to do is find all the points where the derivative goes to 0 and check for which\n",
    "of these points the function has the lowest value\n",
    "\n",
    "Applied to a neural network, that means finding analytically the combination of\n",
    "weight values that yields the smallest possible loss function. This can be done by solving\n",
    "the equation ```gradient(f)(W) = 0``` for ```W```. This is a polynomial equation of ***N*** variables,\n",
    "where N is the number of coefficients in the network. Although it would be\n",
    "possible to solve such an equation for ***N = 2*** or ***N = 3***, doing so is intractable for real\n",
    "neural networks, where the number of parameters is never less than a few thousand\n",
    "and can often be several tens of millions\n",
    "\n",
    " - Draw a batch of training samples ```x``` and corresponding target values ```y```\n",
    " - Run the NN(model) on ```x``` to get ```y_pred```\n",
    " - Calculate the ```loss(y, y_pred)```on the batch\n",
    " - Using ***backward pass**, compute the gradient of the loss with regards to the network parameters\n",
    " - Move the parameters a little in the opposite directionfrom the gradient. ```W -= step*gradient``` -thus reducing the batch loss \n",
    " \n",
    "The above method is called ***Mini-Batch Stochastic Gradient Descent (mini-batch SGD)***. The term ***Stochastic*** means that each batch of the data is drawn at ***random***.\n",
    "\n",
    "![capture](https://user-images.githubusercontent.com/13174586/49491718-57a69180-f87b-11e8-8752-0a2e36e49d78.JPG)\n",
    "\n",
    "It is important to pick a easonable value for ***step*** factor. If is is *too small*, the the gradient descent will take too many iterations to converge and even there is a possibility that it might get stuck at the ***local minimum***. If the ***step*** size is *too large*, it might take the gradient to some random locations on the curve and our network will never converge.\n",
    "\n",
    "There are different variants of SGD:\n",
    "\n",
    " - **True SGD:** Here the algorithm will draw one single sample data from ```x``` and one target data from ```y``` and perform the computations.\n",
    " - **Batch SGD:** Here the algorithm will draw all the data and run all the step on the complete data. Though it is *accurate*, computationally it is very expensive.\n",
    " - **Batch SGD:** Here the algorithm will draw random samples from the data and run all the step on the on them and repeat the process on different samples. This maintains a balance between **True SGD** and **Batch SGD**\n",
    " \n",
    "In addition to the above variants, there are other variants of SGD taht look into the previous weight updates while computing the next weight update. e.g.- SGD with *momentum, Adagrad, RMSProp*. Such variants are known as ***Optimization Method*** or ***Optimizers***.\n",
    "\n",
    "***Momentum*** addresses two issues:\n",
    " - SGD Convergence Speed\n",
    " - Local Minima\n",
    "\n",
    "![capture](https://user-images.githubusercontent.com/13174586/49492661-334cb400-f87f-11e8-9093-415ed26466f9.JPG)\n",
    "\n",
    "Around a certain parameter value, there is a ***local Minima***. From there, if we move to left, the loss will increase. If the ***learning rate/ step*** is small, then moving to the right will also increase the loss and the model may never converge at ***Global Minimum***. To avoid such issues, we can use ***Momentum***. \n",
    "\n",
    "> An real-life representation of optimization process as a small ball rolling down the loss curve. If it has enough momentum, the ball won’t get stuck in a ravine and will end up at the global minimum. Momentum is implemented by moving the ball at each step based not only on the current slope value (current acceleration) but also on the current velocity (resulting from past acceleration). In practice, this means updating the parameter ```w``` based not only on the current gradient value but also on the previous parameter update\n",
    "\n",
    "`past_velocity = 0\n",
    " momentum = 0.1\n",
    " while loss > 0.1:\n",
    "    w, loss, gradient = get_current_parameters()\n",
    "    velocity= past_velocity*momentum + learning_rate*gradient\n",
    "    w = w + momentum*velocity -learning_rate*gradient\n",
    "    past_velocity = velocity\n",
    "    update_parameter(w)`\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### BackPropagation: Chaining Derivative\n",
    "\n",
    "a neural network function consists of many tensor operations chained together, each of which has a simple,\n",
    "known derivative. For instance, this is a network ```f``` composed of three tensor operations,\n",
    "```a```, ```b```, and ```c```, with weight matrices ```W1```, ```W2```, and ```W3```:\n",
    "\n",
    "```f(W1, W2, W3) = a(W1, b(W2, c(W3)))```\n",
    "\n",
    "\n",
    "Calculus tells us that such a chain of functions can be derived using the following identity,\n",
    "called the chain rule: \n",
    "\n",
    "`f(g(x)) = f'(g(x)) * g'(x)`\n",
    "\n",
    "Applying the chain rule algorithm to compute gradient of the neural network is called ***Backpropagation*** or ***Reverse-mode Differentiation***. The backProp starts with the loss from the top layer and and works backward to the bottom layers, applying chain rule to compute contribution of each parameter in the loss value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST Digit Classification\n",
    "\n",
    "The input images are stored in Numpy tensors, which are here formatted as `float32` tensors of shape (60000, 784) (training data) and (10000, 784) (test data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images= train_images.reshape((60000,28*28))\n",
    "train_images= train_images.astype('float32')/255.0\n",
    "\n",
    "test_images= test_images.reshape((10000,28*28))\n",
    "test_images= test_images.astype('float32')/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000,)\n",
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(train_labels.shape)\n",
    "train_labels= to_categorical(train_labels)\n",
    "print(train_labels.shape)\n",
    "test_labels= to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network consists of a chain of two `Dense` layers, that each layer applies a few simple tensor operations to the input data, and that these operations involve weight tensors. Weight tensors, which are attributes of the layers, are where the *knowledge* of the network persists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "network= models.Sequential()\n",
    "network.add(layers.Dense(512, activation='relu', input_shape=(28*28,)))\n",
    "network.add(layers.Dense(10, activation= 'softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Network compilation step:\n",
    "    \n",
    "`categorical_crossentropy` is the *loss function* that’s used as a *feedback signal* for learning the weight tensors, and which the training phase will attempt to minimize. This reduction of the loss happens via *minibatch stochastic gradient descent*. The exact rules governing a specific use of gradient descent are defined by the *rmsprop optimizer* passed as the first argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Traing Network loop:\n",
    "\n",
    "when we call `fit:` the network will start to iterate on the training data in mini-batches of ***128 samples***, ***10*** times \n",
    "over (each iteration over all the training data is called an `epoch`). At each iteration, the network will compute the gradients of the weights with regard to the loss on the batch, and update the weights accordingly. After these ***10 epochs***, the network will have performed ***4690 gradient updates [469 (60000/128) per epoch]***, and the loss of the network will be sufficiently low that the\n",
    "network will be capable of classifying handwritten digits with high accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 11s 184us/step - loss: 0.2592 - acc: 0.9249\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.1034 - acc: 0.9694\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.0686 - acc: 0.9798\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.0501 - acc: 0.9851\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.0378 - acc: 0.9888: 0s - loss: 0.0382 - acc:\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.0283 - acc: 0.9912\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.0218 - acc: 0.9937\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.0175 - acc: 0.9948\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.0133 - acc: 0.9961\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.0106 - acc: 0.9970\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20d9fee11d0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.fit(train_images, train_labels, epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
