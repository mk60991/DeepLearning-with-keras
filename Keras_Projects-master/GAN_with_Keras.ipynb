{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Generative Adversarial Networks (GAN)\n",
    "\n",
    "Generative adversarial networks (GANs), introduced in 2014 by Goodfellow are an alternative to VAEs for learning latent spaces of images. They enable the generation of fairly realistic synthetic images by forcing the generated images to be statistically\n",
    "almost indistinguishable from real ones.\n",
    "\n",
    "An intuitive way to understand GANs is to imagine a forger trying to create a fake Picasso painting. At first, the forger is pretty bad at the task. He mixes some of his fakes with authentic Picassos and shows them all to an art dealer. The art dealer makes an authenticity assessment for each painting and gives the forger feedback about what makes a Picasso look like a Picasso. The forger goes back to his studio to prepare some new fakes. As times goes on, the forger becomes increasingly competent at imitating the style of Picasso, and the art dealer becomes increasingly expert at spotting fakes. In the end, they have on their hands some excellent fake Picassos.\n",
    "\n",
    "That’s what a GAN is: a forger network and an expert network, each being trained to best the other. As such, a GAN is made of two parts:\n",
    " - ***Generator network—*** Takes as input a random vector (a random point in the latent space), and decodes it into a synthetic image\n",
    " - ***Discriminator network (or adversary)—*** Takes as input an image (real or synthetic), and predicts whether the image came from the training set or was created by the generator network.\n",
    "\n",
    "The generator network is trained to be able to fool the discriminator network, and thus it evolves toward generating increasingly realistic images as training goes on: artificial images that look indistinguishable from real ones, to the extent that it’s impossible for the discriminator network to tell the two apart. Meanwhile, the discriminator is constantly adapting to the gradually improving capabilities of the generator, setting a high bar of realism for the generated images. Once training is\n",
    "over, the generator is capable of turning any point in its input space into a believable image. Unlike VAEs, this latent space has fewer explicit guarantees of meaningful structure; in particular, it isn’t continuous.\n",
    "\n",
    "![capture](https://user-images.githubusercontent.com/13174586/52191499-f004c680-286a-11e9-88f0-0842352d5750.JPG)\n",
    "\n",
    "Remarkably, a GAN is a system where the optimization minimum isn’t fixed. Normally, gradient descent consists of rolling down hills in a static loss landscape. But with a GAN, every step taken down the hill changes the entire landscape a little. It’s a dynamic system where the optimization process is seeking not a minimum, but an equilibrium between two forces. For this reason, GANs are notoriously difficult to train—getting a GAN to work requires lots of careful tuning of the model architecture and training parameters.\n",
    "\n",
    "![capture](https://user-images.githubusercontent.com/13174586/52191555-496cf580-286b-11e9-9aab-c4f7cd5be266.JPG)\n",
    "\n",
    "### A Schematic GAN Implementation\n",
    "In this section, we’ll explain how to implement a GAN in Keras, in its barest form. The specific implementation is a deep convolutional GAN (DCGAN): a GAN where the generator and discriminator are *deep convnets*. In particular, it uses a `Conv2DTranspose` layer for image upsampling in the generator.\n",
    "\n",
    "We’ll train the GAN on images from CIFAR10, a dataset of 50,000 32 × 32 RGB images belonging to 10 classes (5,000 images per class). To make things easier, we’ll only use images belonging to the class “frog.”\n",
    "\n",
    "Schematically, the GAN looks like this:\n",
    " - A generator network maps vectors of shape `(latent_dim,)` to images of shape `(32, 32, 3)`.\n",
    " - A discriminator network maps images of shape (32, 32, 3) to a binary score estimating the probability that the image is real.\n",
    " - A gan network chains the generator and the discriminator together: `gan(x) = discriminator(generator(x))`. Thus this gan network maps latent space vectors to the discriminator’s assessment of the realism of these latent vectors as decoded by the generator.\n",
    " - We train the discriminator using examples of real and fake images along with “real”/“fake” labels, just as we train any regular image-classification model.\n",
    " - To train the generator, we use the gradients of the generator’s weights with regard to the loss of the gan model. This means, at every step, we move the weights of the generator in a direction that makes the discriminator more likely to classify as “real” the images decoded by the generator. In other words, we train the generator to fool the discriminator\n",
    " \n",
    "### Tricks for using GANs\n",
    "\n",
    "The process of training GANs and tuning GAN implementations is notoriously difficult. There are a number of known tricks we should keep in mind. Like most things in deep learning, it’s more alchemy than science: these tricks are heuristics, not\n",
    "theory-backed guidelines. They’re supported by a level of intuitive understanding of the phenomenon at hand, and they’re known to work well empirically, although not necessarily in every context.\n",
    "\n",
    "Here are a few of the tricks used in the implementation of the GAN generator and discriminator in this section. It isn’t an exhaustive list of GAN-related tips; we’ll find many more across the GAN literature:\n",
    " - We use `tanh` as the last activation in the generator, instead of `sigmoid`, which is more commonly found in other types of models.\n",
    " - We sample points from the latent space using a `normal distribution (Gaussian distribution)`, not a `uniform distribution`.\n",
    " - Stochasticity is good to induce robustness. Because GAN training results in a dynamic equilibrium, GANs are likely to get stuck in all sorts of ways. Introducing randomness during training helps prevent this. We introduce randomness in two ways: *by using dropout in the discriminator* and *by adding random noise to the labels for the discriminator*.\n",
    " - Sparse gradients can hinder GAN training. In deep learning, sparsity is often a desirable property, but not in GANs. Two things can induce gradient sparsity: `max pooling` operations and `ReLU` activations. Instead of max pooling, we recommend using *strided convolutions for downsampling*, and we recommend using a *`LeakyReLU` layer instead of a `ReLU` activation*. It’s similar to ReLU, but it relaxes sparsity constraints by allowing small negative activation values.\n",
    " - In generated images, it’s common to see checkerboard artifacts caused by unequal coverage of the pixel space in the generator. To fix this, we use a kernel size that’s divisible by the stride size whenever we use a strided `Conv2DTranpose` or `Conv2D` in both the generator and the discriminator.\n",
    " \n",
    "![capture](https://user-images.githubusercontent.com/13174586/52193210-0a8f6d80-2874-11e9-8f87-a6071d70e34f.JPG)\n",
    "\n",
    "### The Generator\n",
    "First, let’s develop a `generator` model that turns a vector (from the latent space— during training it will be sampled at random) into a candidate image. One of the many issues that commonly arise with GANs is that the generator gets stuck with generated images that look like noise. A possible solution is to use dropout on both the discriminator and the generator.\n",
    "\n",
    "### GAN Generator Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32768)             1081344   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 32768)             0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 16, 16, 256)       819456    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 32, 32, 256)       1048832   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 256)       1638656   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 32, 32, 256)       1638656   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 32, 32, 3)         37635     \n",
      "=================================================================\n",
      "Total params: 6,264,579\n",
      "Trainable params: 6,264,579\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "\n",
    "latent_dim=32\n",
    "channels= 3\n",
    "height=32\n",
    "width=32\n",
    "\n",
    "generator_input = keras.Input(shape=(latent_dim,))\n",
    "\n",
    "x= layers.Dense(128*16*16)(generator_input)          #Transforms the input into\n",
    "x= layers.LeakyReLU()(x)                             #a 16 × 16 128-channel\n",
    "x= layers.Reshape((16,16,128))(x)                    #feature map\n",
    "\n",
    "x= layers.Conv2D(256, 5, padding='same')(x)\n",
    "x= layers.LeakyReLU()(x)\n",
    "\n",
    "x= layers.Conv2DTranspose(256, 4, strides=2, padding='same')(x)          #Upsamples\n",
    "x= layers.LeakyReLU()(x)                                                 #to 32 × 32\n",
    "\n",
    "x= layers.Conv2D(256,5, padding='same')(x)\n",
    "x= layers.LeakyReLU()(x)\n",
    "x= layers.Conv2D(256,5, padding='same')(x)\n",
    "x= layers.LeakyReLU()(x)\n",
    "\n",
    "#Produces a 32 × 32 1-channel feature map (shape of a CIFAR10 image)\n",
    "x= layers.Conv2D(channels, 7, activation='tanh', padding='same')(x)\n",
    "generator= keras.models.Model(generator_input, x) #Instantiates the generator model, which maps \n",
    "                                                  #the input of shape (latent_dim,) into an image of shape (32, 32, 3)\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Discriminator\n",
    "Next, we’ll develop a discriminator model that takes as input a candidate image (real or synthetic) and classifies it into one of two classes: “generated image” or “real image that comes from the training set.”\n",
    "\n",
    "### The GAN Discriminator Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 30, 30, 128)       3584      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 30, 30, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 14, 14, 128)       262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 6, 6, 128)         262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 2, 2, 128)         262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 790,913\n",
      "Trainable params: 790,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator_input= layers.Input(shape=(height, width, channels))\n",
    "\n",
    "x= layers.Conv2D(128, 3)(discriminator_input)\n",
    "x= layers.LeakyReLU()(x)\n",
    "x= layers.Conv2D(128, 4, strides=2)(x)\n",
    "x= layers.LeakyReLU()(x)\n",
    "x= layers.Conv2D(128, 4, strides=2)(x)\n",
    "x= layers.LeakyReLU()(x)\n",
    "x= layers.Conv2D(128, 4, strides=2)(x)\n",
    "x= layers.LeakyReLU()(x)\n",
    "x= layers.Flatten()(x)\n",
    "\n",
    "x= layers.Dropout(0.4)(x) #One dropout layer: an important trick!\n",
    "\n",
    "x= layers.Dense(1, activation='sigmoid')(x) #Classification layer\n",
    "\n",
    "discriminator= keras.models.Model(discriminator_input, x) #nstantiates the discriminator model, which turns a (32, 32, 3) input \n",
    "                                                          #into a binary classifi-cation decision (fake/real)\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator_optimizer= keras.optimizers.RMSprop(lr= 0.0008,\n",
    "                                                 clipvalue=1.0, #Uses gradient clipping (by value) in the optimizer\n",
    "                                                 decay=1e-8)    #To stabilize training, uses learning-rate decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.compile(optimizer=discriminator_optimizer, loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Adversarial Network\n",
    "\n",
    "Finally, we’ll set up the GAN, which chains the generator and the discriminator. When trained, this model will move the generator in a direction that improves its ability to fool the discriminator. This model turns latent-space points into a classification decision—“fake” or “real”—and it’s meant to be trained with labels that are always “these are real images.” So, training gan will update the weights of generator in a way that makes discriminator more likely to predict “real” when looking at fake images. It’s very important to note that we set the discriminator to be frozen during training (non-trainable): its weights won’t be updated when training gan. If the discriminator weights could be updated during this process, then we’d be training the discriminator to always predict “real,” which isn’t what we want!\n",
    "\n",
    "### Adversarial Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.trainable= False #Sets discriminator weights to non-trainable (this will only apply to the gan model)\n",
    "\n",
    "gan_input= keras.Input(shape=(latent_dim,))\n",
    "gan_output= discriminator(generator(gan_input))\n",
    "gan= keras.models.Model(gan_input, gan_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_optimizer= keras.optimizers.RMSprop(lr=0.0004, clipvalue=1.0, decay=1e-8)\n",
    "gan.compile(gan_optimizer, loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Train DCGAN\n",
    "\n",
    "Now we can begin training. To recapitulate, this is what the training loop looks like schematically. For each epoch, we do the following:\n",
    " - Draw random points in the latent space (random noise).\n",
    " - Generate images with generator using this random noise.\n",
    " - Mix the generated images with real ones.\n",
    " - Train discriminator using these mixed images, with corresponding targets: either “real” (for the real images) or “fake” (for the generated images).\n",
    " - Draw new random points in the latent space.\n",
    " - Train gan using these random vectors, with targets that all say “these are real images.” This updates the weights of the generator (only, because the discriminator is frozen inside gan) to move them toward getting the discriminator to predict “these are real images” for generated images: this trains the generator to fool the discriminator.\n",
    "\n",
    "\n",
    "### Implement GAN Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from keras.preprocessing import image\n",
    "\n",
    "(x_train, y_train), (_,_)= keras.datasets.cifar10.load_data()  #Loads CIFAR10 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train= x_train[y_train.flatten()==7] #store horses\n",
    "\n",
    "x_train= x_train.reshape((x_train.shape[0],)+ (height, width, channels)).astype('float32')/255.0 #Normalizes data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations=10000\n",
    "batch_size=20\n",
    "save_dir= 'GAN_images'  #Specifies where we want to save generated images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\soumyama\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discriminator loss: 0.6617702\n",
      "adversarial_loss: 0.68149436\n",
      "discriminator loss: 0.69179994\n",
      "adversarial_loss: 0.7019526\n",
      "discriminator loss: 0.69367456\n",
      "adversarial_loss: 0.74562055\n",
      "discriminator loss: 0.6808406\n",
      "adversarial_loss: 0.7324556\n",
      "discriminator loss: 0.6894037\n",
      "adversarial_loss: 0.7302474\n",
      "discriminator loss: 0.6715142\n",
      "adversarial_loss: 1.2362105\n",
      "discriminator loss: 0.69639766\n",
      "adversarial_loss: 0.75128037\n",
      "discriminator loss: 0.6497221\n",
      "adversarial_loss: 1.1229557\n",
      "discriminator loss: 0.6870656\n",
      "adversarial_loss: 0.7571552\n",
      "discriminator loss: 0.7025717\n",
      "adversarial_loss: 0.74121445\n",
      "discriminator loss: 0.70527303\n",
      "adversarial_loss: 0.72985065\n",
      "discriminator loss: 0.6891298\n",
      "adversarial_loss: 0.78328544\n",
      "discriminator loss: 0.70616543\n",
      "adversarial_loss: 0.7662188\n",
      "discriminator loss: 0.72330284\n",
      "adversarial_loss: 0.8188443\n",
      "discriminator loss: 0.8074007\n",
      "adversarial_loss: 0.7552217\n",
      "discriminator loss: 0.6813648\n",
      "adversarial_loss: 0.7658\n",
      "discriminator loss: 0.6993841\n",
      "adversarial_loss: 0.7587241\n",
      "discriminator loss: 0.6856198\n",
      "adversarial_loss: 0.7173792\n",
      "discriminator loss: 0.7024656\n",
      "adversarial_loss: 0.7409746\n",
      "discriminator loss: 0.6865734\n",
      "adversarial_loss: 0.77735436\n",
      "discriminator loss: 0.6795065\n",
      "adversarial_loss: 0.7473971\n",
      "discriminator loss: 0.68935823\n",
      "adversarial_loss: 0.72939193\n",
      "discriminator loss: 0.695508\n",
      "adversarial_loss: 0.9387314\n",
      "discriminator loss: 0.68264085\n",
      "adversarial_loss: 0.7881298\n",
      "discriminator loss: 0.7144636\n",
      "adversarial_loss: 0.7093822\n",
      "discriminator loss: 0.69615066\n",
      "adversarial_loss: 0.70702255\n",
      "discriminator loss: 0.6650204\n",
      "adversarial_loss: 0.86215067\n",
      "discriminator loss: 0.6894657\n",
      "adversarial_loss: 0.73396266\n",
      "discriminator loss: 0.71878034\n",
      "adversarial_loss: 0.79285544\n",
      "discriminator loss: 0.686504\n",
      "adversarial_loss: 0.77241385\n",
      "discriminator loss: 0.6988284\n",
      "adversarial_loss: 0.77327883\n",
      "discriminator loss: 0.6926618\n",
      "adversarial_loss: 0.912306\n",
      "discriminator loss: 0.6873288\n",
      "adversarial_loss: 0.72219145\n",
      "discriminator loss: 0.69611514\n",
      "adversarial_loss: 0.7995404\n",
      "discriminator loss: 0.6924575\n",
      "adversarial_loss: 0.7416196\n",
      "discriminator loss: 0.690462\n",
      "adversarial_loss: 0.7397858\n",
      "discriminator loss: 0.7080291\n",
      "adversarial_loss: 0.7438916\n",
      "discriminator loss: 0.6943716\n",
      "adversarial_loss: 0.7718366\n",
      "discriminator loss: 0.69987917\n",
      "adversarial_loss: 0.8518793\n",
      "discriminator loss: 0.69550794\n",
      "adversarial_loss: 0.76458246\n",
      "discriminator loss: 0.7164885\n",
      "adversarial_loss: 0.764458\n",
      "discriminator loss: 0.69936335\n",
      "adversarial_loss: 0.7512351\n",
      "discriminator loss: 0.7105785\n",
      "adversarial_loss: 0.7732283\n",
      "discriminator loss: 0.6928232\n",
      "adversarial_loss: 0.76118696\n",
      "discriminator loss: 0.68655795\n",
      "adversarial_loss: 0.7307858\n",
      "discriminator loss: 0.6858894\n",
      "adversarial_loss: 0.7474934\n",
      "discriminator loss: 0.6844737\n",
      "adversarial_loss: 0.78682184\n",
      "discriminator loss: 0.7048899\n",
      "adversarial_loss: 0.8432525\n",
      "discriminator loss: 0.7079174\n",
      "adversarial_loss: 0.8008467\n",
      "discriminator loss: 0.6960753\n",
      "adversarial_loss: 0.7604147\n",
      "discriminator loss: 0.695731\n",
      "adversarial_loss: 0.76503\n",
      "discriminator loss: 0.7098303\n",
      "adversarial_loss: 0.7773572\n",
      "discriminator loss: 0.67530006\n",
      "adversarial_loss: 0.7531138\n",
      "discriminator loss: 0.7083807\n",
      "adversarial_loss: 0.8123703\n",
      "discriminator loss: 0.68220115\n",
      "adversarial_loss: 0.76000035\n",
      "discriminator loss: 0.6792059\n",
      "adversarial_loss: 0.7255339\n",
      "discriminator loss: 0.6992005\n",
      "adversarial_loss: 0.73363024\n",
      "discriminator loss: 0.6766939\n",
      "adversarial_loss: 0.94046336\n",
      "discriminator loss: 0.7077316\n",
      "adversarial_loss: 0.6920265\n",
      "discriminator loss: 0.68224066\n",
      "adversarial_loss: 0.7739214\n",
      "discriminator loss: 0.689274\n",
      "adversarial_loss: 0.76881886\n",
      "discriminator loss: 0.69538957\n",
      "adversarial_loss: 0.75213134\n",
      "discriminator loss: 0.67944854\n",
      "adversarial_loss: 0.7189665\n",
      "discriminator loss: 0.69416445\n",
      "adversarial_loss: 0.6578113\n",
      "discriminator loss: 0.70144194\n",
      "adversarial_loss: 0.77355546\n",
      "discriminator loss: 0.72070736\n",
      "adversarial_loss: 0.7001053\n",
      "discriminator loss: 0.69145757\n",
      "adversarial_loss: 0.70566356\n",
      "discriminator loss: 0.71253717\n",
      "adversarial_loss: 0.82791233\n",
      "discriminator loss: 0.7609423\n",
      "adversarial_loss: 0.8625849\n",
      "discriminator loss: 0.6806685\n",
      "adversarial_loss: 0.6910962\n",
      "discriminator loss: 0.65976226\n",
      "adversarial_loss: 0.74652237\n",
      "discriminator loss: 0.70031166\n",
      "adversarial_loss: 0.76011926\n",
      "discriminator loss: 0.69336855\n",
      "adversarial_loss: 0.77201176\n",
      "discriminator loss: 0.6876764\n",
      "adversarial_loss: 0.7424687\n",
      "discriminator loss: 0.6982237\n",
      "adversarial_loss: 0.76187325\n",
      "discriminator loss: 0.6897196\n",
      "adversarial_loss: 0.81455773\n",
      "discriminator loss: 0.74290264\n",
      "adversarial_loss: 0.8464837\n",
      "discriminator loss: 0.67769253\n",
      "adversarial_loss: 0.86103725\n",
      "discriminator loss: 0.68810433\n",
      "adversarial_loss: 0.8171029\n",
      "discriminator loss: 0.6866416\n",
      "adversarial_loss: 0.7661084\n",
      "discriminator loss: 0.6544279\n",
      "adversarial_loss: 0.7977182\n",
      "discriminator loss: 0.6493448\n",
      "adversarial_loss: 0.9114087\n",
      "discriminator loss: 0.66261625\n",
      "adversarial_loss: 0.9120986\n",
      "discriminator loss: 0.69561976\n",
      "adversarial_loss: 0.7829577\n",
      "discriminator loss: 0.69118136\n",
      "adversarial_loss: 0.7710906\n",
      "discriminator loss: 0.6693016\n",
      "adversarial_loss: 0.77962387\n",
      "discriminator loss: 0.6985997\n",
      "adversarial_loss: 0.77077246\n",
      "discriminator loss: 0.6860237\n",
      "adversarial_loss: 0.78774464\n",
      "discriminator loss: 0.7240037\n",
      "adversarial_loss: 0.86490315\n",
      "discriminator loss: 0.7095102\n",
      "adversarial_loss: 0.73172176\n",
      "discriminator loss: 0.6994714\n",
      "adversarial_loss: 0.82643116\n",
      "discriminator loss: 0.72335756\n",
      "adversarial_loss: 0.71914136\n",
      "discriminator loss: 0.7068163\n",
      "adversarial_loss: 0.7934413\n",
      "discriminator loss: 0.7524152\n",
      "adversarial_loss: 0.89652836\n",
      "discriminator loss: 0.68931067\n",
      "adversarial_loss: 0.84482366\n",
      "discriminator loss: 0.69726485\n",
      "adversarial_loss: 0.7654873\n",
      "discriminator loss: 0.692716\n",
      "adversarial_loss: 0.7806768\n",
      "discriminator loss: 0.707823\n",
      "adversarial_loss: 0.7122542\n",
      "discriminator loss: 0.6860297\n",
      "adversarial_loss: 0.5700178\n",
      "discriminator loss: 0.68226117\n",
      "adversarial_loss: 0.7791981\n"
     ]
    }
   ],
   "source": [
    "start=0\n",
    "for step in range(iterations):\n",
    "    random_latent_vectors= np.random.normal(size=(batch_size, latent_dim)) #Samples random points in the latent space\n",
    "    \n",
    "    generated_images= generator.predict(random_latent_vectors) #Decodes them to fake images\n",
    "    \n",
    "    stop= start+batch_size\n",
    "    real_images= x_train[start:stop]                                 #Combines them\n",
    "    combined_images= np.concatenate([generated_images, real_images]) #with real images\n",
    "    \n",
    "    labels= np.concatenate([np.ones((batch_size,1)),  #Assembles labels, discriminating\n",
    "                           np.zeros((batch_size,1))]) #real from fake images\n",
    "    \n",
    "    labels+= 0.05*np.random.random(labels.shape) #Adds random noise to the labels—an important trick!\n",
    "    \n",
    "    d_loss= discriminator.train_on_batch(combined_images, labels) #Trains the discriminator\n",
    "    \n",
    "    random_latent_vectors= np.random.normal(size=(batch_size, latent_dim)) #Samples random points in the latent space\n",
    "    \n",
    "    misleading_targets= np.zeros((batch_size, 1)) #Assembles latent space labels that say \n",
    "                                                  #“these are all real images” (it’s a lie!)\n",
    "        \n",
    "    a_loss= gan.train_on_batch(random_latent_vectors, misleading_targets) #Trains the generator (via the gan model, \n",
    "                                                                          #where the discriminator weights are frozen)\n",
    "        \n",
    "    start += batch_size\n",
    "    if start> len(x_train) -batch_size:\n",
    "        start=0\n",
    "        \n",
    "    if step %100 == 0:            #Occasionally saves and plots (every 100 steps)\n",
    "        gan.save_weights('gan.h5')#Saves model weights\n",
    "        \n",
    "        print('discriminator loss:', d_loss)  #Prints \n",
    "        print('adversarial_loss:', a_loss)    #metrics\n",
    "        \n",
    "        img= image.array_to_img(generated_images[0]*255., scale=False)\n",
    "        img.save(os.path.join(save_dir, 'generated_horse'+str(step)+'.png')) #Saves one generated image\n",
    "        img= image.array_to_img(real_images[0]*255., scale=False)\n",
    "        img.save(os.path.join(save_dir, 'real_horse'+str(step)+'.png'))      #Saves one real image for comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training, we may see the adversarial loss begin to increase considerably, while the discriminative loss tends to zero—the discriminator may end up dominating the generator. If that’s the case, try reducing the discriminator learning rate, and increase the dropout rate of the discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
