{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a straightforward approach to map natural language to intents in the form of labels. So the whole effort really is in first converting the NL input into vectors and training an appropriate network which understands arbitrary variations.\n",
    "\n",
    "Mapping input to intends is a common approach to chat-bots in order to present the appropriate response. For example, there are many ways to request a train ticket but it all maps to the same purchase procedure. So, this kinda learning attempts to handle all ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import SimpleRNN\n",
    "from keras.layers.core import Dense, Dropout, Flatten\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers import Convolution1D, LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import itertools\n",
    "import numpy as np\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data weâ€™ll use in the limited case is as follows:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = [\"What would it cost to travel to the city on Monday?\",\n",
    "         \"Need to travel this afternoon\",\n",
    "         \"I want to buy a ticket\",\n",
    "         \"Can I order a trip?\", \n",
    "         \"I would like to buy a ticket to Brussels\", \n",
    " \n",
    "         \"What will be the weather tomorrow?\",\n",
    "         \"Will it rain this afternoon?\",\n",
    "         \"The sunshine feels great\",\n",
    "         \"Can you predict rain?\",\n",
    "         \"Guess I should wear a jacket hey!\",\n",
    " \n",
    "        \"Dit is geheel iets anders\",\n",
    "         \"Kan ik dit goed vinden\",\n",
    "         \"Wat is dit soms goed\",\n",
    "        \"Maar anders is soms goed\"]\n",
    " \n",
    "T = \"Buy a train ticket\"\n",
    "W = \"Asking about the weather\"\n",
    "F = \"Babble in 't Vlaamsch\"\n",
    "\n",
    "labelsTrain = [T,\n",
    "               T,\n",
    "               T,\n",
    "               T,\n",
    "               T,\n",
    " \n",
    "               W,\n",
    "               W,\n",
    "               W,\n",
    "               W,\n",
    "               W,\n",
    " \n",
    "               F,\n",
    "               F,\n",
    "               F,\n",
    "               F]\n",
    " \n",
    "test = [\n",
    "        \"Do you think it will be sunny tomorrow?\",\n",
    "        \"What a wonderful feeling in the sun!\",\n",
    "        \"How can I travel to Leuven?\",\n",
    "        \"Can I buy it from you?\",\n",
    "        \"Anders is heel goed\"\n",
    "       ]\n",
    "labelsTest = [W, W, T, T, F]\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data is constrained in the sense that the testing questions use only words which were trained. In this case we can use the Keras Tokenizer class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'to': 1, 'i': 2, 'a': 3, 'it': 4, 'the': 5, 'can': 6, 'is': 7, 'goed': 8, 'what': 9, 'travel': 10, 'buy': 11, 'will': 12, 'you': 13, 'dit': 14, 'anders': 15, 'would': 16, 'this': 17, 'afternoon': 18, 'ticket': 19, 'be': 20, 'tomorrow': 21, 'rain': 22, 'soms': 23, 'cost': 24, 'city': 25, 'on': 26, 'monday': 27, 'need': 28, 'want': 29, 'order': 30, 'trip': 31, 'like': 32, 'brussels': 33, 'weather': 34, 'sunshine': 35, 'feels': 36, 'great': 37, 'predict': 38, 'guess': 39, 'should': 40, 'wear': 41, 'jacket': 42, 'hey': 43, 'geheel': 44, 'iets': 45, 'kan': 46, 'ik': 47, 'vinden': 48, 'wat': 49, 'maar': 50, 'do': 51, 'think': 52, 'sunny': 53, 'wonderful': 54, 'feeling': 55, 'in': 56, 'sun': 57, 'how': 58, 'leuven': 59, 'from': 60, 'heel': 61}\n",
      "\n",
      "Length of all text: 19\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "all_texts = train + test # Combines two lists\n",
    "tokenizer.fit_on_texts(all_texts) \n",
    "print(tokenizer.word_index)\n",
    "print(\"\\nLength of all text:\",len(all_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "texts_to_matrix method -> to convert the sentences directly to equal size arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tokenizer.texts_to_matrix(train) # Convert the sentences directly to equal size arrays\n",
    "X_test = tokenizer.texts_to_matrix(test)   # Convert the sentences directly to equal size arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What would it cost to travel to the city on Monday?', 'Need to travel this afternoon', 'I want to buy a ticket', 'Can I order a trip?', 'I would like to buy a ticket to Brussels', 'What will be the weather tomorrow?', 'Will it rain this afternoon?', 'The sunshine feels great', 'Can you predict rain?', 'Guess I should wear a jacket hey!', 'Dit is geheel iets anders', 'Kan ik dit goed vinden', 'Wat is dit soms goed', 'Maar anders is soms goed']\n",
      "\n",
      "Shape of X_train: (14, 62)\n",
      "Shpae of X_test: (5, 62)\n"
     ]
    }
   ],
   "source": [
    "print(train)\n",
    "print(\"\\nShape of X_train:\",X_train.shape)\n",
    "print(\"Shpae of X_test:\",X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Displaying first 2 rows of X_train:\n",
    "for i in range(0,2):\n",
    "    print(X_train[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The labels are converted to index vectors in order to use categorical crossentropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Buy a train ticket', 'Asking about the weather', \"Babble in 't Vlaamsch\"]\n"
     ]
    }
   ],
   "source": [
    "all_labels = labelsTest + labelsTrain\n",
    "labels = set(all_labels) # set -> function avoid duplicates\n",
    "idx2labels = list(labels)\n",
    "print(idx2labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Buy a train ticket': 0, 'Asking about the weather': 1, \"Babble in 't Vlaamsch\": 2}\n"
     ]
    }
   ],
   "source": [
    "# The enumerate() function adds a counter to an iterable.\n",
    "label2idx = dict((v, i) for i, v in enumerate(labels)) # dictionary holding key and value\n",
    "print(label2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical([label2idx[w] for w in labelsTrain])\n",
    "y_test = to_categorical([label2idx[w] for w in labelsTest])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 0 0 2 "
     ]
    }
   ],
   "source": [
    "for i in labelsTest:\n",
    "    print(label2idx[i],end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: The `dropout` argument is no longer support in `Embedding`. You can apply a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get the same behavior.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 80.00%\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    " \n",
    "model = Sequential()\n",
    "model.add(Embedding(2, 45, input_length= X_train.shape[1], dropout=0.2 ))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, name='middle'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(3, activation='softmax', name='output')) \n",
    " \n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    " \n",
    "model.fit(X_train, y=y_train, epochs=1500, verbose=0, validation_split=0.2, shuffle=True)\n",
    " \n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even without sophisticated RNN you get all the accuracy you can wish. Note that despite the lack of non-learned words you can still get results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(tokenizer.texts_to_matrix([\"Welke dag is het vandaag?\"])).round()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using word2vec and LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you can find networks on the basis of word2vec and LSTM.\n",
    "There are a few factors:\n",
    "\n",
    "- the training data is measure zero\n",
    "- the pretrained word embedding is in English, so the Flemish language is not embedded and hence not learned\n",
    "- there is no temporal correlation and the learning is not used towards predicting the next word or something like this, so LSTM is not very meaningful here\n",
    "\n",
    "Still, an accuracy of 20% while the plain dense network gives 100% is surprising. Definitely a proof that â€˜moreâ€™ is not always automatically better in this field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "# see here to download the pretrained model\n",
    "# http://nlp.stanford.edu/projects/glove/\n",
    "glove_data = './data/glove.6B/glove.6B.50d.txt'\n",
    "f = open(glove_data,encoding='UTF-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    value = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = value\n",
    "f.close()\n",
    " \n",
    "print('Loaded %s word vectors.' % len(embeddings_index))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'to': 1, 'i': 2, 'a': 3, 'it': 4, 'the': 5, 'can': 6, 'is': 7, 'goed': 8, 'what': 9, 'travel': 10, 'buy': 11, 'will': 12, 'you': 13, 'dit': 14, 'anders': 15, 'would': 16, 'this': 17, 'afternoon': 18, 'ticket': 19, 'be': 20, 'tomorrow': 21, 'rain': 22, 'soms': 23, 'cost': 24, 'city': 25, 'on': 26, 'monday': 27, 'need': 28, 'want': 29, 'order': 30, 'trip': 31, 'like': 32, 'brussels': 33, 'weather': 34, 'sunshine': 35, 'feels': 36, 'great': 37, 'predict': 38, 'guess': 39, 'should': 40, 'wear': 41, 'jacket': 42, 'hey': 43, 'geheel': 44, 'iets': 45, 'kan': 46, 'ik': 47, 'vinden': 48, 'wat': 49, 'maar': 50, 'do': 51, 'think': 52, 'sunny': 53, 'wonderful': 54, 'feeling': 55, 'in': 56, 'sun': 57, 'how': 58, 'leuven': 59, 'from': 60, 'heel': 61}\n"
     ]
    }
   ],
   "source": [
    "embedding_dimension = 50 # Setting embedding dimension to 50\n",
    "word_index = tokenizer.word_index \n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dimension)) # Initiallizing the embedding matrix with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector[:embedding_dimension]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(embedding_matrix.shape[0],\n",
    "                            embedding_matrix.shape[1],\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=len(word_index) + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9, 16, 4, 24, 1, 10, 1, 5, 25, 26, 27], [28, 1, 10, 17, 18], [2, 29, 1, 11, 3, 19], [6, 2, 30, 3, 31], [2, 16, 32, 1, 11, 3, 19, 1, 33], [9, 12, 20, 5, 34, 21], [12, 4, 22, 17, 18], [5, 35, 36, 37], [6, 13, 38, 22], [39, 2, 40, 41, 3, 42, 43], [14, 7, 44, 45, 15], [46, 47, 14, 8, 48], [49, 7, 14, 23, 8], [50, 15, 7, 23, 8]]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "X_train = tokenizer.texts_to_sequences(train) # Which Turns The input into numerical arrays \n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of word_index: 61\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of word_index:\",len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  9 16  4 24  1 10  1  5 25 26 27]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0 28  1 10 17 18]\n"
     ]
    }
   ],
   "source": [
    "X_train = pad_sequences(X_train, maxlen=len(word_index) + 1) \n",
    "# pad_sequence takes a LIST of sequences as an input (list of list) and will return a list of padded sequences.\n",
    "\n",
    "# First 2 rows of X_train\n",
    "for i in range(0,2):\n",
    "    print(X_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 40.00%\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='sigmoid'))\n",
    "model.layers[0].trainable=False # bug in Keras or Theano\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(3, activation='softmax', name='output')) \n",
    " \n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    " \n",
    "model.fit(X_train, y=y_train, epochs=2500, verbose=0, validation_split=0.2, shuffle=True)\n",
    " \n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(128, dropout=0.2, recurrent_dropout=0.2)`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\models.py:874: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 40.00%\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(LSTM(128, dropout_W=0.2, dropout_U=0.2)) \n",
    "model.add(Dense(3, activation='softmax', name='output')) \n",
    " \n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    " \n",
    "model.fit(X_train, y=y_train, nb_epoch=1000, verbose=0, validation_split=0.2, shuffle=True)\n",
    " \n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
