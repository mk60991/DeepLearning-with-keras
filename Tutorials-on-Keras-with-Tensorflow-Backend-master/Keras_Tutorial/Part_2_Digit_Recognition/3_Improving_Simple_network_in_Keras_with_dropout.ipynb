{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Improving the Neural Network with Dropout\n",
    "1. We decide to randromly drop with dropout probability some of value propogated inside our internal dense network of hidden layers.\n",
    "2. In ML-> This is a well known form of regularization.\n",
    "3. Idea of dropping few values can improve performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential    # Importing Sequential Model\n",
    "from keras.layers.core import Dense,Dropout, Activation  #  Importing  Dense Layers,Dropouts and Activation functions\n",
    "from keras.optimizers import SGD  # Importing SGD(Stochastic Gradient Descent) optimizer\n",
    "from keras.utils import np_utils  \n",
    "np.random.seed(1671) # for reproducibility -> Once you put the same seed you get same patterns of random numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network and training\n",
    "NB_EPOCH = 20  # 20-> times the model is exposed to the training set.\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "OPTIMIZER = SGD() # SGD optimizer, explained later in this chapter\n",
    "N_HIDDEN = 128 # Neurons\n",
    "VALIDATION_SPLIT=0.2 # how much TRAIN is reserved for VALIDATION\n",
    "DROPOUT = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data: shuffled and split between train and test sets\n",
    "\n",
    "(X_train, y_train_label), (X_test, y_test_label) = mnist.load_data()\n",
    "\n",
    "#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
    "\n",
    "RESHAPED = 784\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test  =  X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# normalize -> Involve only rescaling to arrive at value relative to some size variables.\n",
    "\n",
    "X_train /= 255 # Pixel values are 0 to 255 -> So we are normalizing training data by dividing it by 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class vectors to binary class matrices\n",
    "Y_train_label = np_utils.to_categorical(y_train_label, NB_CLASSES) \n",
    "Y_test_label = np_utils.to_categorical(y_test_label, NB_CLASSES)\n",
    "\n",
    "# np_utils.to_categorical Used to convert the array of labelled data to one Hot vector-> Binarization of category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Final hidden layer  with 10 outputs\n",
    "# final stage is softmax\n",
    "model = Sequential() # Sequential Model.\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,))) # 1st Hidden Layer --> 128 neurons and input dimension ->784\n",
    "model.add(Activation('relu')) # Activation function for 1st Hidden Layer\n",
    "model.add(Dropout(DROPOUT))\n",
    "\n",
    "model.add(Dense(N_HIDDEN))  # 2nd Hidden Layer --> 128 neurons\n",
    "model.add(Activation('relu')) # Activation function for 2nd Hidden Layer\n",
    "model.add(Dropout(DROPOUT))\n",
    "\n",
    "\n",
    "model.add(Dense(NB_CLASSES)) # Final layer with 10 neurons == > no of outputs\n",
    "model.add(Activation('softmax')) # Final layer activation will be 'softmax'\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling a model in keras\n",
    "model.compile(loss='categorical_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 3s 56us/step - loss: 1.7404 - acc: 0.4539 - val_loss: 0.9293 - val_acc: 0.8124\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.9232 - acc: 0.7229 - val_loss: 0.5401 - val_acc: 0.8653\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 2s 48us/step - loss: 0.6935 - acc: 0.7882 - val_loss: 0.4298 - val_acc: 0.8884\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 2s 46us/step - loss: 0.5947 - acc: 0.8209 - val_loss: 0.3790 - val_acc: 0.8977\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 2s 50us/step - loss: 0.5347 - acc: 0.8392 - val_loss: 0.3456 - val_acc: 0.9040\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 2s 49us/step - loss: 0.4976 - acc: 0.8523 - val_loss: 0.3232 - val_acc: 0.9107\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - ETA: 0s - loss: 0.4627 - acc: 0.862 - 2s 51us/step - loss: 0.4616 - acc: 0.8628 - val_loss: 0.3048 - val_acc: 0.9128\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 2s 44us/step - loss: 0.4386 - acc: 0.8688 - val_loss: 0.2896 - val_acc: 0.9170\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 2s 51us/step - loss: 0.4181 - acc: 0.8761 - val_loss: 0.2776 - val_acc: 0.9199\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 2s 47us/step - loss: 0.3990 - acc: 0.8838 - val_loss: 0.2657 - val_acc: 0.9233\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.3819 - acc: 0.8875 - val_loss: 0.2551 - val_acc: 0.9257\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 2s 50us/step - loss: 0.3688 - acc: 0.8920 - val_loss: 0.2465 - val_acc: 0.9283\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 2s 50us/step - loss: 0.3571 - acc: 0.8943 - val_loss: 0.2388 - val_acc: 0.9299\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 2s 47us/step - loss: 0.3466 - acc: 0.8992 - val_loss: 0.2319 - val_acc: 0.9323\n",
      "Epoch 15/20\n",
      "22528/48000 [=============>................] - ETA: 1s - loss: 0.3365 - acc: 0.9022"
     ]
    }
   ],
   "source": [
    "# Training a model in keras\n",
    "\n",
    "# Once the model is compiled it can be trained with the fit() function\n",
    "\n",
    "history = model.fit(X_train, Y_train_label,\n",
    "batch_size=BATCH_SIZE, epochs=NB_EPOCH,\n",
    "verbose=VERBOSE, validation_split=VALIDATION_SPLIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When Training end we achieve <b>94.56</b>--><b>Accuracy on Training</b>\n",
    "- <b>94.97</b>--><b>Accuracy on Validataion</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally calucating the score.\n",
    "score = model.evaluate(X_test, Y_test_label, verbose=VERBOSE)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tested our model with <b>Test dataset</b> and achieved accuracy of <b>94.6</b>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
